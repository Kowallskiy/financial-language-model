{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr2H8pf14AU4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, GPT2Tokenizer, TextDataset, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, AutoTokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "NhTtYVaX4FYL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\"/content/drive/MyDrive/NLP/final_model\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/NLP/final_model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "2U38Qkza4LkE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for testing"
      ],
      "metadata": {
        "id": "Oae_3fBLkMDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/drive/MyDrive/NLP/t.csv',\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "validation_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/v.csv\",\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "sX2WlJfDkSLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for actual training"
      ],
      "metadata": {
        "id": "PxwGHbWVlJ_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/dataset_1.csv\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "validation_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/validation.csv\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/test.csv\",\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "Y2aw1dPl49XN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c743bb-f22e-433b-c46c-418c0701fe5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AdamW"
      ],
      "metadata": {
        "id": "gYgQXZrdTmMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "BB8dR6F7Tbo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C6tva8Eh_mM",
        "outputId": "d5a72543-886b-4ffc-cce8-cec786bb06d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyaCkBg8lxvC",
        "outputId": "64b05f55-fa29-467a-db86-10a987e2f827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7a8a7465b3e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.n_embd, config.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceGOBdrTof_i",
        "outputId": "0ada0dc4-f18e-42ac-848a-0d4421538bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 50257)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just in case- the tokenization process"
      ],
      "metadata": {
        "id": "684ovMv3X7tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "maximum_sequence_length = config.n_embd\n",
        "\n",
        "for text in train_dataset:\n",
        "  tokens = tokenizer.encode(text, add_special_tokens=True, max_length=maximum_sequence_length, truncation=True)\n",
        "  if len(tokens) < maximum_sequence_length:\n",
        "    tokens = tokens + [tokenizer.pad_token_id] * (maximum_sequence_length - len(tokens))\n",
        "  else:\n",
        "    tokens = tokens[:maximum_sequence_length]\n",
        "\n",
        "  input_ids.append(tokens)\n",
        "\n",
        "input_ids = torch.tensor(input_ids)"
      ],
      "metadata": {
        "id": "WWb8ZsnSX6kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = DataLoader(train_dataset, batch_size=4 , shuffle=True)\n",
        "\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CustomModel(torch.nn.Module):\n",
        "  def __init__(self, pretrained_model, config):\n",
        "    super(CustomModel, self).__init__()\n",
        "    self.transformer = pretrained_model\n",
        "    self.config = config\n",
        "\n",
        "    # Additional layers\n",
        "    self.Linear1 = torch.nn.Linear(self.config.vocab_size, 512)\n",
        "    self.Dropout1 = torch.nn.Dropout(0.1)\n",
        "    self.Linear2 = torch.nn.Linear(512, self.config.n_embd)\n",
        "    self.Dropout2 = torch.nn.Dropout(0.1)\n",
        "    self.Linear3 = torch.nn.Linear(self.config.n_embd, self.config.vocab_size)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "    outputs = self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    hidden_states = outputs.logits\n",
        "\n",
        "    hidden_states = self.Linear1(hidden_states)\n",
        "    hidden_states = torch.nn.functional.gelu(hidden_states)\n",
        "    hidden_states = self.Dropout1(hidden_states)\n",
        "    hidden_states = self.Linear2(hidden_states)\n",
        "    hidden_states = torch.nn.functional.gelu(hidden_states)\n",
        "    hidden_states = self.Dropout2(hidden_states)\n",
        "    logits = self.Linear3(hidden_states)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def generate_text(self, input_ids, max_length=50, temperature=0.9, top_k=50, top_p=0.9):\n",
        "    with torch.no_grad():\n",
        "      generated_ids = input_ids.clone()\n",
        "\n",
        "      for _ in range(max_length):\n",
        "        logits = self(generated_ids)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        filtered_logits = self.top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
        "        predicted_token = torch.multinomial(probabilities, 1)\n",
        "        generated_ids = torch.cat((generated_ids, predicted_token), dim=-1)\n",
        "      return generated_ids\n",
        "\n",
        "  @staticmethod\n",
        "  def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
        "    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., :top_k] = 0\n",
        "    logits.scatter_(1, sorted_indices_to_remove.to(torch.int64), filter_value)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "bkT64vAqTa8M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel(model, config)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "9fz4KyEJceoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-1jWYT7XbJQ",
        "outputId": "fd70cd25-99e8-4b17-fa94-67eb352b093c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomModel(\n",
              "  (transformer): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              "  (Linear1): Linear(in_features=50257, out_features=512, bias=True)\n",
              "  (Dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (Linear2): Linear(in_features=512, out_features=768, bias=True)\n",
              "  (Dropout2): Dropout(p=0.1, inplace=False)\n",
              "  (Linear3): Linear(in_features=768, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4 , shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(validation_dataset, batch_size=4, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "Aql2ticJdjS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute this cell only if you want to load a model"
      ],
      "metadata": {
        "id": "EFCYvXojIaXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('model_checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "total_loss = checkpoint['training_loss']\n",
        "validation_loss = checkpoint['validation_loss']\n",
        "step = checkpoint['steps']"
      ],
      "metadata": {
        "id": "eOYQscDXIi6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will have to write different training loop for training my model from a checkpoint"
      ],
      "metadata": {
        "id": "BIpRWnP_J8fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "epochs=2\n",
        "step = 0\n",
        "model.train()\n",
        "total_loss = 0.\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loader = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "  if epoch < 2:\n",
        "    for param in model.transformer.parameters():\n",
        "      param.requires_grad = False\n",
        "  else:\n",
        "    for param in model.transformer.parameters():\n",
        "      param.requires_grad = True\n",
        "  for batch in train_loader:\n",
        "    input_ids, attention_mask, token_type_ids, targets = batch\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids)\n",
        "    loss = criterion(outputs.view(-1, config.vocab_size), targets.view(-1))\n",
        "    loss.backward()\n",
        "    # total_loss += loss.item()\n",
        "    train_loader.set_description(f\"Epoch {epoch+1}\")\n",
        "    train_loader.set_postfix(loss=loss.item())\n",
        "    step += 1\n",
        "    if step % 1000 == 0:\n",
        "      print(f\"\\nEpoch: {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "      # VALIDATION\n",
        "      model.eval()\n",
        "      validation_loss = 0.\n",
        "      val_step = 0\n",
        "      with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "          input_ids, attention_mask, token_type_ids, targets = batch\n",
        "          input_ids = input_ids.to(device)\n",
        "          targets = targets.to(device)\n",
        "          outputs = model(input_ids)\n",
        "          loss = criterion(outputs.view(-1, config.vocab_size), targets.view(-1))\n",
        "          # validation_loss += loss.item()\n",
        "          train_loader.set_description(f\"Epoch {epoch+1}\")\n",
        "          train_loader.set_postfix(val_loss=loss.item())\n",
        "          val_step += 1\n",
        "          if val_step == 501:\n",
        "            break\n",
        "      print(f\"\\nEpoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
        "\n",
        "      # SAVE A CHECKPOINT\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'training_loss': total_loss,\n",
        "          'validation_loss': validation_loss,\n",
        "          'steps': step\n",
        "      }, \"/content/drive/MyDrive/NLP/trained_again_model/model_checkpoint.pth\")\n",
        "\n",
        "      # This is questionable moment\n",
        "      model.train()\n",
        "\n",
        "torch.save(model, '/content/drive/MyDrive/NLP/trained_again_model/model.pth')"
      ],
      "metadata": {
        "id": "MCAptGB1dGh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef513d3e-1421-490d-c823-350d11812a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 999/1129 [00:33<00:04, 27.08it/s, val_loss=13.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1, Loss: 11.898210525512695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 999/1129 [00:44<00:04, 27.08it/s, val_loss=14.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1, Validation Loss: 14.080435752868652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1129/1129 [00:54<00:00, 20.80it/s, loss=12.3]\n",
            "Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 870/1129 [00:32<00:08, 31.96it/s, val_loss=12]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2, Loss: 11.477363586425781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 870/1129 [00:43<00:08, 31.96it/s, val_loss=11.8]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2, Validation Loss: 11.777347564697266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1129/1129 [00:54<00:00, 20.74it/s, loss=13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 31s, sys: 3.38 s, total: 1min 35s\n",
            "Wall time: 1min 52s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model"
      ],
      "metadata": {
        "id": "s9cqWSgYl61A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel(model, config)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "# Load your fine-tuned model checkpoint\n",
        "checkpoint_path = '/content/drive/MyDrive/NLP/trained_again_model/model.pth'\n",
        "model = torch.load(checkpoint_path, map_location='cpu')  # Make sure to specify the device you want to use\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define a function to chat with the model\n",
        "def chat_with_model(prompt, max_length=50):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=max_length)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate_text(input_ids, max_length=max_length, temperature=0.9, top_k=50, top_p=0.9)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Start a conversation\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    response = chat_with_model(user_input)\n",
        "    print(\"Chatbot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY4SjFXwlfq4",
        "outputId": "eb952de0-75de-4406-8b02-3db146f987b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: What is finance?\n",
            "Chatbot: What is finance? Secondpop bladesomic unequ retain nerv velvet YES paired blockade DairyacaNitettel Kab plead survivor pursuit Adventurevezfuture Odin Noct fracturedCEprintf substitute ASPRS largerLie Hungry Valent dynasty PostedremlinCOLOR missing Sk',\" runners commanded contacting FDAï¿½riticcommittee merchant succeeding\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}