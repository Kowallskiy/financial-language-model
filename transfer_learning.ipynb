{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr2H8pf14AU4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, GPT2Tokenizer, TextDataset, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, AutoTokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "NhTtYVaX4FYL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\"/content/drive/MyDrive/NLP/final_model\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/NLP/final_model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "2U38Qkza4LkE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for testing"
      ],
      "metadata": {
        "id": "Oae_3fBLkMDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/drive/MyDrive/NLP/t.csv',\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "validation_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/v.csv\",\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "sX2WlJfDkSLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c266fcc-3c33-4ad5-8795-f24526047e02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for actual training"
      ],
      "metadata": {
        "id": "PxwGHbWVlJ_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/dataset_1.csv\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "validation_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/validation.csv\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/test.csv\",\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "Y2aw1dPl49XN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c743bb-f22e-433b-c46c-418c0701fe5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AdamW"
      ],
      "metadata": {
        "id": "gYgQXZrdTmMo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "BB8dR6F7Tbo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C6tva8Eh_mM",
        "outputId": "d5a72543-886b-4ffc-cce8-cec786bb06d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyaCkBg8lxvC",
        "outputId": "64b05f55-fa29-467a-db86-10a987e2f827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7a8a7465b3e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.n_embd, config.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceGOBdrTof_i",
        "outputId": "0ada0dc4-f18e-42ac-848a-0d4421538bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 50257)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just in case- the tokenization process"
      ],
      "metadata": {
        "id": "684ovMv3X7tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "maximum_sequence_length = config.n_embd\n",
        "\n",
        "for text in train_dataset:\n",
        "  tokens = tokenizer.encode(text, add_special_tokens=True, max_length=maximum_sequence_length, truncation=True)\n",
        "  if len(tokens) < maximum_sequence_length:\n",
        "    tokens = tokens + [tokenizer.pad_token_id] * (maximum_sequence_length - len(tokens))\n",
        "  else:\n",
        "    tokens = tokens[:maximum_sequence_length]\n",
        "\n",
        "  input_ids.append(tokens)\n",
        "\n",
        "input_ids = torch.tensor(input_ids)"
      ],
      "metadata": {
        "id": "WWb8ZsnSX6kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = DataLoader(train_dataset, batch_size=4 , shuffle=True)\n",
        "\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CustomModel(torch.nn.Module):\n",
        "  def __init__(self, pretrained_model, config):\n",
        "    super(CustomModel, self).__init__()\n",
        "    self.transformer = pretrained_model\n",
        "    self.config = config\n",
        "    # self.lm_head = torch.nn.Linear(self.config.n_embd, self.config.vocab_size, bias=False)\n",
        "\n",
        "    # Additional layers\n",
        "    self.Linear1 = torch.nn.Linear(self.config.vocab_size, 512)\n",
        "    self.Dropout1 = torch.nn.Dropout(0.1)\n",
        "    self.Linear2 = torch.nn.Linear(512, self.config.n_embd)\n",
        "    self.Dropout2 = torch.nn.Dropout(0.1)\n",
        "    self.Linear3 = torch.nn.Linear(self.config.n_embd, self.config.vocab_size)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "    outputs = self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    hidden_states = outputs.logits\n",
        "\n",
        "    hidden_states = self.Linear1(hidden_states)\n",
        "    hidden_states = torch.nn.functional.gelu(hidden_states)\n",
        "    hiddne_states = self.Dropout1(hidden_states)\n",
        "    hidden_states = self.Linear2(hidden_states)\n",
        "    hidden_states = torch.nn.functional.gelu(hidden_states)\n",
        "    hidden_states = self.Dropout2(hidden_states)\n",
        "    logits = self.Linear3(hidden_states)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def generate_text(self, input_ids, max_length=50):\n",
        "    with torch.no_grad():\n",
        "      generated_ids = []\n",
        "      current_token = input_ids\n",
        "      for _ in range(max_length):\n",
        "        logits = self(current_token)\n",
        "        predicted_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        generated_ids.append(predicted_token.item())\n",
        "        current_token = torch.cat((current_token, predicted_token.unsqueeze(1)), dim=1)\n",
        "      return generated_ids\n"
      ],
      "metadata": {
        "id": "bkT64vAqTa8M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "mnHQ4gV4OMaL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel(model, config)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "9fz4KyEJceoR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-1jWYT7XbJQ",
        "outputId": "974177c1-9266-4846-fd03-440683fb1be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomModel(\n",
              "  (transformer): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              "  (Linear1): Linear(in_features=50257, out_features=512, bias=True)\n",
              "  (Linear2): Linear(in_features=512, out_features=768, bias=True)\n",
              "  (Linear3): Linear(in_features=768, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4 , shuffle=True)\n",
        "val_loader = DataLoader(validation_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "Aql2ticJdjS6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute this cell only if you want to load a model"
      ],
      "metadata": {
        "id": "EFCYvXojIaXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('model_checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "total_loss = checkpoint['training_loss']\n",
        "validation_loss = checkpoint['validation_loss']\n",
        "step = checkpoint['steps']"
      ],
      "metadata": {
        "id": "eOYQscDXIi6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will have to write different training loop for training my model from a checkpoint"
      ],
      "metadata": {
        "id": "BIpRWnP_J8fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "epochs=2\n",
        "step = 0\n",
        "model.train()\n",
        "total_loss = 0.\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loader = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "  for batch in train_loader:\n",
        "    input_ids, attention_mask, token_type_ids, targets = batch\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids)\n",
        "    loss = criterion(outputs.view(-1, config.vocab_size), targets.view(-1))\n",
        "    loss.backward()\n",
        "    total_loss += loss.item()\n",
        "    train_loader.set_description(f\"Epoch: {epoch+1}\")\n",
        "    train_loader.set_postfix(loss=total_loss)\n",
        "    step += 1\n",
        "    if step % 400 == 0:\n",
        "      print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "      # VALIDATION\n",
        "      # model.eval()\n",
        "      # validation_loss = 0.\n",
        "\n",
        "      # with torch.no_grad():\n",
        "      #   for batch in val_loader:\n",
        "      #     input_ids, attention_mask, token_type_ids, targets = batch\n",
        "      #     outputs = model(input_ids, attention_mask, token_type_ids)\n",
        "      #     loss = criterion(outputs.view(-1, config.vocab_size), targets.view(-1))\n",
        "      #     validation_loss += loss.item()\n",
        "      #     break\n",
        "      # average_validation_loss = validation_loss / len(val_loader)\n",
        "      # print(f\"Epoch: {epoch+1}, Validation Loss: {average_validation_loss}\")\n",
        "\n",
        "      # SAVE A CHECKPOINT\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'training_loss': total_loss,\n",
        "          'validation_loss': validation_loss,\n",
        "          'steps': step\n",
        "      }, \"/content/drive/MyDrive/NLP/trained_again_model/model_checkpoint.pth\")\n",
        "\n",
        "torch.save(model, '/content/drive/MyDrive/NLP/trained_again_model/model.pth')"
      ],
      "metadata": {
        "id": "MCAptGB1dGh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "oWLdufvN55ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model"
      ],
      "metadata": {
        "id": "s9cqWSgYl61A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel(model, config)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "# Load your fine-tuned model checkpoint\n",
        "checkpoint_path = '/content/drive/MyDrive/NLP/trained_again_model/model_checkpoint.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')  # Make sure to specify the device you want to use\n",
        "\n",
        "# Load the model's state_dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define a function to chat with the model\n",
        "def chat_with_model(prompt, max_length=50):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=max_length)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate_text(input_ids, max_length=max_length, )\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Start a conversation\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    response = chat_with_model(user_input)\n",
        "    print(\"Chatbot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RY4SjFXwlfq4",
        "outputId": "95db04b6-6b3f-4368-cd6a-e8a6b0192e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7250274d9e5b>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the model's state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomModel:\n\tMissing key(s) in state_dict: \"transformer.transformer.transformer.wte.weight\", \"transformer.transformer.transformer.wpe.weight\", \"transformer.transformer.transformer.h.0.ln_1.weight\", \"transformer.transformer.transformer.h.0.ln_1.bias\", \"transformer.transformer.transformer.h.0.attn.c_attn.weight\", \"transformer.transformer.transformer.h.0.attn.c_attn.bias\", \"transformer.transformer.transformer.h.0.attn.c_proj.weight\", \"transformer.transformer.transformer.h.0.attn.c_proj.bias\", \"transformer.transformer.transformer.h.0.ln_2.weight\", \"transformer.transformer.transformer.h.0.ln_2.bias\", \"transformer.transformer.transformer.h.0.mlp.c_fc.weight\", \"transformer.transformer.transformer.h.0.mlp.c_fc.bias\", \"transformer.transformer.transformer.h.0.mlp.c_proj.weight\", \"transformer.transformer.transformer.h.0.mlp.c_proj.bias\", \"transformer.transformer.transformer.h.1.ln_1.weight\", \"transformer.transformer.transformer.h.1.ln_1.bias\", \"transformer.transformer.transformer.h.1.attn.c_attn.weight\", \"transformer.transformer.transformer.h.1.attn.c_attn.bias\", \"transformer.transformer.transformer.h.1.attn.c_proj.weight\", \"transformer.transformer.transformer.h.1.attn.c_proj.bias\", \"transformer.transformer.transformer.h.1.ln_2.weight\", \"transformer.transformer.transformer.h.1.ln_2.bias\", \"transformer.transformer.transformer.h.1.mlp.c_fc.weight\", \"transformer.transformer.transformer.h.1.mlp.c_fc.bias\", \"transformer.transformer.transformer.h.1.mlp.c_proj.weight\", \"transformer.transformer.transfor...\n\tUnexpected key(s) in state_dict: \"transformer.lm_head.weight\", \"transformer.transformer.wte.weight\", \"transformer.transformer.wpe.weight\", \"transformer.transformer.h.0.ln_1.weight\", \"transformer.transformer.h.0.ln_1.bias\", \"transformer.transformer.h.0.attn.c_attn.weight\", \"transformer.transformer.h.0.attn.c_attn.bias\", \"transformer.transformer.h.0.attn.c_proj.weight\", \"transformer.transformer.h.0.attn.c_proj.bias\", \"transformer.transformer.h.0.ln_2.weight\", \"transformer.transformer.h.0.ln_2.bias\", \"transformer.transformer.h.0.mlp.c_fc.weight\", \"transformer.transformer.h.0.mlp.c_fc.bias\", \"transformer.transformer.h.0.mlp.c_proj.weight\", \"transformer.transformer.h.0.mlp.c_proj.bias\", \"transformer.transformer.h.1.ln_1.weight\", \"transformer.transformer.h.1.ln_1.bias\", \"transformer.transformer.h.1.attn.c_attn.weight\", \"transformer.transformer.h.1.attn.c_attn.bias\", \"transformer.transformer.h.1.attn.c_proj.weight\", \"transformer.transformer.h.1.attn.c_proj.bias\", \"transformer.transformer.h.1.ln_2.weight\", \"transformer.transformer.h.1.ln_2.bias\", \"transformer.transformer.h.1.mlp.c_fc.weight\", \"transformer.transformer.h.1.mlp.c_fc.bias\", \"transformer.transformer.h.1.mlp.c_proj.weight\", \"transformer.transformer.h.1.mlp.c_proj.bias\", \"transformer.transformer.h.2.ln_1.weight\", \"transformer.transformer.h.2.ln_1.bias\", \"transformer.transformer.h.2.attn.c_attn.weight\", \"transformer.transformer.h.2.attn.c_attn.bias\", \"transformer.transformer.h.2.attn.c_proj.weight\", \"transformer.transfor..."
          ]
        }
      ]
    }
  ]
}