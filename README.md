# financial-language-model
___
## Описание
Данный репозиторий представляет собой проект, направленный на разработку NLP модели, специализированной в финансовой сфере. Модель была fine-tuned на предобученной модели gpt2 с использованием разнообразных источников данных, включая Reddit, Wikipedia, Investopedia, и финансовые книги. Большая часть данных была получена путем парсинга финансовых ресурсов.

__Цели разработки проекта__:
* __Изучение методов генераци текста__: Проект направлен на исследование и понимание методов генерации текста с использованием NLP моделей.
* __Обучение модели на финансовых данных__: Проект предполагает обучение на специфических финансовых данных, что представляет свои собственные технические и лингвистические сложности.
* __Оценка производительности__: Важной частью проекта является оценка производительности моедли после обучения, а также выявление возможных ограничений.
## Процесс обучения
__Выбор модели и токенизатора__:

Для обучения были выбраны модель и токенизатор _GPT-2_ ввиду способности модели выполнять широкий спектр задач в рамках естественного языка, включая генерацию текста. Также _GPT-2_ предоставляет возможность настройки параметров и и создания различных конфигураций модели.
```Python
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
```
__Подготовка данных__:

Загружены train_dataset, validation_dataset и test_dataset. Для обучения модели использовались следующие параметры:
```Python
training_args = TrainingArguments(
    output_dir = "/content/drive/MyDrive/NLP/model",
    overwrite_output_dir = False,
    num_train_epochs = 5,
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 2,
    save_steps = 600,
    save_total_limit = 2,
    logging_dir = "/content/drive/MyDrive/NLP/logs",
    save_strategy = 'steps',
    evaluation_strategy = 'steps',
    eval_steps = 600,
    logging_steps = 100,
    do_train = True,
    do_eval = True,
    load_best_model_at_end = True,
    remove_unused_columns = True
)
```
__Обнаружение и решение проблем__:

В процессе обучения возникла проблема с overfitting, когда validation loss начал постепенно расти, в то время как training loss уменьшался. 
|__Steps__|__Training Loss__|__Validation Loss__|
|------------|-------------|-----------|
|6000|3.470900|3.738574|
|6600|3.457300|3.727550|
|7200|3.374900|3.733540|
|7800|3.373100|3.740214|
|8400|3.363600|3.746500|
|------|-------|-------|

Обучение было остановлено вручную, и num_train_epochs был уменьшен до 3, чтобы избежать overfitting. Данную проблему можно устранить функцией EarlyStoppingCallback() из Hugging Face's Transformers библиотеки.

Также, было обнаружено, что training_dataset включал в себя преимущественно данные с Reddit, в то время как validation_dataset содержал как данные с Reddit, так и Wikipedia. Это привело к тому, что модель запомнила стиль Reddit, что отразилось на validation loss.

После устранения ошибок и обучения, возникла ещё одна серьёзная проблема. Обучение модели проводилось частями из-за ограничения использования Google Colab GPU. Тестирование показало, что после каждого нового курга обучения модель забывала предыдущую информацию.

__Борьба с "catastrophic forgetting"__:

Чтобы снизить влияние "catastrophic forgetting", были применены техники Dropout и Weight Decay (L2 Regularization):
```Python
# Dropout
model.config.attention_dropout = 0.1
model.config.hidden_dropout_prob = 0.1

# Weight Decay
training_args = TrainingArguments(
    ...
    weight_decay = 0.01
)
```
__Результаты__:

После всех настроек и улучшений, модель продемонстрировала некоторые успехи в генерации текста в финансовой сфере. Однако, производительность модели остается недостаточной.
## Демонстрация модели

## Инструменты и технологии
* Python
* Hugging Face transformers
* Pytorch
## Вывод
В процессе разработки данного проекта были сделаны следующие выводы:
* __Значение разнообразия данных__: Для эффективного обучения модели генерации текста, было не только важно правильно настроить параметры, но и иметь доступ к обширному разнообразию данных. Данные из разных источников, такие как Reddit, Wikipedia, Investopedia и финансовые книги, оказались весьма ценными, но недостаточными для эффективного обучения модели. 
* __Управление обучением и overfitting__: Важной составляющей разработки было управление процессом обучения. Необходимость в снижении overfitting требовала внимательного подхода к выбору параметров и методов. Это демонстрирует, что успешная разработка NLP модели не только требует технической экспертизы, но и опыта в управлении обучением.
* __Разделение данных__: Качественное разделение данных на тренировочные, валидационные и тестовые наборы данных оказалось фундаментальным для понимания производительности модели. Этот процесс помог не только в обучении модели, но и в оценке её эффективности.
* __Эффект catastrophic interference__: Эффект катастрофического забывания является неизбежной особенностью обучения нейронных сетей, что требует принятия мер для уменьшения данного эффекта.
