{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRecxY-j2hbT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, GPT2Tokenizer, TextDataset, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, AutoTokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "8leMwuUI21gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\"/content/drive/MyDrive/NLP/final_model\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/NLP/final_model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "kXKZuiJD24An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path = \"/content/drive/MyDrive/NLP/dataset_1.csv\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "validation_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/NLP/v.csv\",\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7spFzVy2_TH",
        "outputId": "cdfd2f11-9070-4e3b-812e-66863808b888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import tqdm\n",
        "\n",
        "def get_fisher_diag(model, dataset, params, empirical=True):\n",
        "  fisher = {}\n",
        "  params_dict = dict(params)\n",
        "  for n, p in deepcopy(params_dict).items():\n",
        "    p.data.zero_()\n",
        "    fisher[n] = p.data.clone().detach().requires_grad_()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  dataset = tqdm(dataset, total=len(dataset))\n",
        "\n",
        "  for batch in dataset:\n",
        "    input, _, _, target = batch\n",
        "\n",
        "    input = input.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "    output = model(input)\n",
        "    output = output.logits\n",
        "    output = output.view(-1, output.size(-1))\n",
        "    # output = model(input).view(1, -1)\n",
        "    if empirical:\n",
        "      label = target.view(-1)\n",
        "    else:\n",
        "      label = torch.argmax(output, dim=1)\n",
        "\n",
        "    cross_entropy_loss = torch.nn.functional.cross_entropy(output, label)\n",
        "    cross_entropy_loss.backward()\n",
        "\n",
        "    for n, p in model.named_parameters():\n",
        "      fisher[n].data += p.grad.data ** 2 / len(dataset)\n",
        "\n",
        "  fisher = {n: p for n, p in fisher.items()}\n",
        "  return fisher\n",
        "\n",
        "def get_ewc_loss(model, fisher, p_old):\n",
        "  loss = 0\n",
        "  for n, p in model.named_parameters():\n",
        "    _loss = fisher[n] * (p - p_old[n]) ** 2\n",
        "    loss += _loss.sum()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "0xAuWmP92v-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "o54PdIeN6fE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
        "eval_dataloader = DataLoader(validation_dataset, batch_size=4, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "j0ZPlQpICXAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "9T8SkVeVI1zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "fisher_matrix = get_fisher_diag(model, train_dataloader, model.named_parameters())\n",
        "prev_params = {n: p.data.clone() for n, p in model.named_parameters()}\n",
        "\n",
        "learning_rate = 0.001\n",
        "ewc_lambda = 0.1\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model.train()\n",
        "  total_loss = 0.\n",
        "\n",
        "  train_dataloader = tqdm(train_dataloader, total=len(train_dataloader))\n",
        "\n",
        "  for batch in train_dataloader:\n",
        "    input, _, _, target = batch\n",
        "\n",
        "    input = input.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # print(target.shape)\n",
        "    output = model(input)\n",
        "    output = output.logits\n",
        "    output = output.view(-1, output.size(-1))\n",
        "\n",
        "    label = target.view(-1)\n",
        "\n",
        "    # Original loss\n",
        "    ce_loss = torch.nn.functional.cross_entropy(output, label)\n",
        "\n",
        "    # EWC loss\n",
        "    ewc_loss = get_ewc_loss(model, fisher_matrix, prev_params)\n",
        "\n",
        "    loss = ce_loss + ewc_lambda * ewc_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_dataloader.set_description(f\"Epoch {epoch+1}\")\n",
        "    train_dataloader.set_postfix(loss=loss.item())\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  # Update fisher matrix and previous parameters after each epoch\n",
        "  if epoch < 2:\n",
        "    fisher_matrix = get_fisher_diag(model, train_dataloader, model.named_parameters())\n",
        "    prev_params = {n: p.data.clone() for n, p in model.named_parameters()}\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss}\")\n",
        "\n",
        "  # Validation\n",
        "  model.eval()\n",
        "  val_loss = 0.\n",
        "  with torch.no_grad():\n",
        "\n",
        "    eval_dataloader = tqdm(eval_dataloader, total=len(eval_dataloader))\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "      val_input, _, _, val_target = batch\n",
        "\n",
        "      val_input = val_input.to(device)\n",
        "      val_target = val_target.to(device)\n",
        "\n",
        "      output_val = model(val_input)\n",
        "      output_val = output_val.logits\n",
        "      output_val = output_val.view(-1, output_val.size(-1))\n",
        "      label_val = val_target.view(-1)\n",
        "\n",
        "      eval_dataloader.set_description(f\"Epoch {epoch+1}\")\n",
        "\n",
        "      val_loss += torch.nn.functional.cross_entropy(output_val, label_val).item()\n",
        "\n",
        "  avg_val_loss = val_loss / len(eval_dataloader)\n",
        "  print(f\"Epoch: {epoch+1}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "  # Save a chekpoint\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict()\n",
        "  }, \"/content/drive/MyDrive/NLP/EWC_model/model_checkpoint.pth\")\n",
        "\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/NLP/EWC_model/model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "eixvpZYo7f-S",
        "outputId": "018c849d-7380-4518-f5f2-ffe584ec6cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20387/20387 [22:18<00:00, 15.23it/s]\n",
            "  0%|          | 0/20387 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithCrossAttentions' object has no attribute 'view'"
          ]
        }
      ]
    }
  ]
}