{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlOJv3XjDOI1"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoModel, GPT2Tokenizer, TextDataset, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, AutoTokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "zlX13M3JDhJY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\"/content/drive/MyDrive/NLP/model\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/NLP/model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "id": "ir-SV_CkEGbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.attention_dropout = 0.1\n",
        "model.config.hidden_dropout_prob = 0.1"
      ],
      "metadata": {
        "id": "2SMGwZoSHtLY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = \"/content/drive/MyDrive/NLP/mitig_train.txt\",\n",
        "    block_size = 128\n",
        ")"
      ],
      "metadata": {
        "id": "YroHvFjXF82y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = TextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = \"/content/drive/MyDrive/NLP/validation.txt\",\n",
        "    block_size = 128\n",
        ")"
      ],
      "metadata": {
        "id": "t-OfLtC9GPjs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = \"/content/drive/MyDrive/NLP/test.txt\",\n",
        "    block_size = 128\n",
        ")"
      ],
      "metadata": {
        "id": "3nEMurnVGYAR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer = tokenizer, mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "GaHitnOwGhrh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"/content/drive/MyDrive/NLP/NEW_model\",\n",
        "    overwrite_output_dir = False,\n",
        "    num_train_epochs = 3,\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 2,\n",
        "    save_steps = 600,\n",
        "    save_total_limit = 2,\n",
        "    logging_dir = \"/content/drive/MyDrive/NLP/logs\",\n",
        "    save_strategy = 'steps',\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps = 600,\n",
        "    logging_steps = 100,\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    load_best_model_at_end = True,\n",
        "    remove_unused_columns = True,\n",
        "    weight_decay = 0.01, # L2 Regularization\n",
        "    warmup_steps = 1000\n",
        ")"
      ],
      "metadata": {
        "id": "I0nNiNatG-Nk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = validation_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "dS3VICRbHJrP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "wfOV4KvVAJej",
        "outputId": "d717f597-60e8-4910-9757-5b481d1ac1b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9435' max='9435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9435/9435 46:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.401700</td>\n",
              "      <td>3.717812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.408600</td>\n",
              "      <td>3.743768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.373500</td>\n",
              "      <td>3.743501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>3.235900</td>\n",
              "      <td>3.743263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.392000</td>\n",
              "      <td>3.738752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>3.107600</td>\n",
              "      <td>3.749642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>3.235600</td>\n",
              "      <td>3.742400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>3.118600</td>\n",
              "      <td>3.739563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>3.105900</td>\n",
              "      <td>3.736210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.200400</td>\n",
              "      <td>3.735053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>3.040000</td>\n",
              "      <td>3.747566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>3.074600</td>\n",
              "      <td>3.747775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>3.010300</td>\n",
              "      <td>3.747003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>2.984700</td>\n",
              "      <td>3.741898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>3.078800</td>\n",
              "      <td>3.744311</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9435, training_loss=3.197336012734644, metrics={'train_runtime': 2785.8463, 'train_samples_per_second': 13.547, 'train_steps_per_second': 3.387, 'total_flos': 2465290321920000.0, 'train_loss': 3.197336012734644, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EWC"
      ],
      "metadata": {
        "id": "YZLLQflAzWN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = [{'name': 'task', 'dataset_path': '/content/drive/MyDrive/NLP/mitig_train.txt'}]"
      ],
      "metadata": {
        "id": "Jgie0N25zZtw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing EWC-specific variables\n",
        "ewc_lambda = 0.1\n",
        "prev_model = model.state_dict()\n",
        "fisher_information = {}"
      ],
      "metadata": {
        "id": "DtOsnhgQ0LUg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task in tasks:\n",
        "  # Set the model in training mode\n",
        "  model.train()\n",
        "\n",
        "  # Calculate the task specific loss\n",
        "  for step, batch in enumerate(train_dataset):\n",
        "    inputs, labels = batch\n",
        "    outputs = model(**inputs)\n",
        "    task_loss = outputs.loss()\n",
        "\n",
        "    # Calcualte gradients\n",
        "    task_loss.backward()\n",
        "\n",
        "    # Update Fisher information\n",
        "    for name, param in model.named_parameters():\n",
        "      if name in fisher_information:\n",
        "        fisher_information[name] += (param.grad.data ** 2).mean()\n",
        "      else:\n",
        "        fisher_information[name] = (param.grad.data ** 2).mean()\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Add EWC regularization term to the loss\n",
        "    for name, param in model.named_parameters():\n",
        "      ewc_term = (fisher_information[name] * (param - prev_model[name]).pow(2)).sum()\n",
        "      task_loss += (ewc_lamda / 2) * ewc_term\n",
        "\n",
        "    # Backpropagate and optimize\n",
        "    task_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # Update the previous model\n",
        "  prev_model = model.state_dict()\n",
        "\n"
      ],
      "metadata": {
        "id": "6YrTivGz0vCe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "8ce0eaba-706b-4c90-a9e9-acbe8b720bf4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7f5628454c25>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Calculate the task specific loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtask_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "CbODl7FPHdX4",
        "outputId": "4df3dfb1-7823-40b0-cdd8-e90c02d9d0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9435' max='9435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9435/9435 41:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.407900</td>\n",
              "      <td>3.726959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.408600</td>\n",
              "      <td>3.746835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.371800</td>\n",
              "      <td>3.744811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>3.234500</td>\n",
              "      <td>3.743347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.390900</td>\n",
              "      <td>3.739234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>3.105100</td>\n",
              "      <td>3.752472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>3.229900</td>\n",
              "      <td>3.745833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>3.113700</td>\n",
              "      <td>3.740594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>3.101300</td>\n",
              "      <td>3.738008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.195100</td>\n",
              "      <td>3.736427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>3.037900</td>\n",
              "      <td>3.748967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>3.074200</td>\n",
              "      <td>3.749033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>3.010100</td>\n",
              "      <td>3.748440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>2.983500</td>\n",
              "      <td>3.743384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>3.076700</td>\n",
              "      <td>3.745835</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9435, training_loss=3.195666524121922, metrics={'train_runtime': 2499.5264, 'train_samples_per_second': 15.099, 'train_steps_per_second': 3.775, 'total_flos': 2465290321920000.0, 'train_loss': 3.195666524121922, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate(eval_dataset=test_dataset)"
      ],
      "metadata": {
        "id": "oFI3IXsOQcsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "F8RokZnhQwuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('/content/drive/MyDrive/NLP/final_model')"
      ],
      "metadata": {
        "id": "y_NZMo2zQzvQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Test the model__"
      ],
      "metadata": {
        "id": "QKlKtylzQ4nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/drive/MyDrive/NLP/final_model\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "def chat_with_model():\n",
        "  while True:\n",
        "    user_input = input(\"You: \")\n",
        "    input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
        "\n",
        "    response_ids = model.generate(input_ids, max_length=100, num_return_sequences=1,\n",
        "                                  temperature=0.9, top_k=15, do_sample=True,\n",
        "                                  pad_token_id=model.config.eos_token_id)\n",
        "\n",
        "    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    print(f\"Model: {response}\")\n",
        "\n",
        "chat_with_model()"
      ],
      "metadata": {
        "id": "g2hAAzRnQ7wO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}